{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syntec-research/Cafca/blob/main/Cafca_Synthetic_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cafca Synthetic Dataset"
      ],
      "metadata": {
        "id": "0Iaq4ION0UYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Information"
      ],
      "metadata": {
        "id": "u17eRANh0hnm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh6jBVXz9Bo2"
      },
      "source": [
        "\n",
        "This notebook visualizes the main components of the Cafca synthetic dataset. The full dataset contains 1,500 subjects, each of which is rendered with 13 expressions in 3 environments from 30 views, resulting in 1.755 Mio. images.\n",
        "\n",
        "### Folder Structure\n",
        "\n",
        "The paths follow this format: `SUBJECT`/`EXP`_`ENV`/`{cameras_json, color_image, foreground_mask, segmentation}`\n",
        "\n",
        "`SUBJECT` and `EXP` are 5-digit numbers. `ENV` is a 3-digit number. Camera names go from `C00` to `C29`.\n",
        "\n",
        "Please see \"Utilities\" (`load_example(...)`) for loading a scene.\n",
        "\n",
        "\n",
        "### Environments\n",
        "The dataset contains each expression rendered in three different environments. The first environment (index `000`) is the same for all expressions (`Laval_Indoor_9C4A5690_8k.exr`). The other two environments are picked at random from the [Laval Indoor Dataset](http://indoor.hdrdb.com/). The environment name is saved in `environment.json` in the `color_image` folder.\n",
        "\n",
        "\n",
        "## Coordinate System\n",
        "\n",
        "The dataset uses a right-handed coordinate system following OpenCV convention (X right, Y down, Z forward). For convenience, the camera json file contains redundant information:\n",
        "the full projection matrices (`P`), extrinsic and intrinsic matrices (`world2cam` / `cam2world` and `K`), and all of\n",
        "these parameters individually.\n",
        "\n",
        "## Segmentation\n",
        "\n",
        "The segmentations are stored as grayscale 16-bit PNG. Region `0` is the background, and other entries refer to regions like facial skin, throat, hair, upper body, eyes, etc.\n",
        "\n",
        "## FLAME 2023 Annotations and Keypoints\n",
        "\n",
        "The dataset includes pseudo-GT annotations for keypoints and [FLAME](https://flame.is.tue.mpg.de/\") 2023. These annotations were obtained by running [VHAP](https://github.com/ShenhanQian/VHAP), a photometric fitting pipeline.\n",
        "\n",
        "The annotations are stored as `npz` files that combine all expressions for a particular frame and environment. For example, the FLAME parameters for subject `00000` and environment `000` are stored under\n",
        "`00000-00099/00000/flame_2023_000/tracked_flame_params_100.npz`.\n",
        "\n",
        "2D Keypoints are estimated with [STAR](https://github.com/ShenhanQian/STAR/) and stored as `npz` in the `landmark2d/STAR` subfolder for each frame and camera. For example, the following path stores landmarks for subject `00000`, expression `00001`, environment `000` and camera `C00`: `00000-00099/00000/00001_000/landmark2d/STAR/C00.npz`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "VqNvPc4D0w8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and Unzip Sample Dataset\n",
        "print('Downloading sample dataset with 5 identities...')\n",
        "!wget https://dataset.ait.ethz.ch/downloads/cafca_v2/mini_sample_dataset.zip\n",
        "!unzip mini_sample_dataset.zip"
      ],
      "metadata": {
        "id": "lTAG2bvQDZZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapy trimesh --quiet"
      ],
      "metadata": {
        "id": "4KBYnM2vE0bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt_dI-oiuux-"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import plotly.io as pio\n",
        "\n",
        "import mediapy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import plotly.graph_objs as go\n",
        "from typing import List, Any\n",
        "import trimesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zqBvzk3uyy2"
      },
      "outputs": [],
      "source": [
        "#@title Utilities\n",
        "\n",
        "def read_camera_json(path: str) -> dict[str, Any]:\n",
        "  \"\"\"Returns a dict with camera parameters.\"\"\"\n",
        "  with open(path, 'rb') as f:\n",
        "    return json.load(f)\n",
        "\n",
        "\n",
        "def read_foreground_mask(path: str) -> np.ndarray:\n",
        "  \"\"\"Returns a float foreground mask file in [0, 1.0].\"\"\"\n",
        "  foreground_mask = mediapy.read_image(path).astype(float)\n",
        "  # Map from [0, 255] to [0, 1].\n",
        "  return foreground_mask / 255.0\n",
        "\n",
        "\n",
        "def scatter3d(arr: np.ndarray | List, name: str, mode: str='markers', **kwargs) -> go.Scatter3d:\n",
        "  \"\"\"Returns a plotly.graph_objs.Scatter3d object for the given array.\"\"\"\n",
        "  arr = np.array(arr)\n",
        "  return go.Scatter3d(x=arr[:, 0], y=arr[:, 1], z=arr[:, 2], mode=mode, name=name, **kwargs)\n",
        "\n",
        "\n",
        "def read_flame_mesh(path: str) -> trimesh.Trimesh:\n",
        "  return trimesh.load(path)\n",
        "\n",
        "def read_flame_params(path: str) -> dict[str, Any]:\n",
        "  params = np.load(path)\n",
        "  keys = params.files\n",
        "  return {k: params[k] for k in keys}\n",
        "\n",
        "def load_example(base_dir, subject: str, expression: str, environment: str, camera_name: str) -> dict[str, Any]:\n",
        "  \"\"\"Returns a dict with data for an individual scene.\"\"\"\n",
        "  frame_dir = os.path.join(base_dir, subject, f'{expression}_{environment}')\n",
        "\n",
        "  rgb = mediapy.read_image(os.path.join(frame_dir, f'color_image/{camera_name}.png'))\n",
        "  segmentation = mediapy.read_image(os.path.join(frame_dir, f'segmentation/{camera_name}.png'))\n",
        "  foreground_mask = read_foreground_mask(os.path.join(frame_dir, f'foreground_mask/{camera_name}.png'))\n",
        "  camera = read_camera_json(os.path.join(frame_dir, f'cameras_json/{camera_name}.json'))\n",
        "  flame_mesh = read_flame_mesh(os.path.join(base_dir, subject, f'flame_2023_{environment}/eval_100/mesh/frame_{expression}.obj'))\n",
        "  flame_params = read_flame_params(os.path.join(base_dir, subject, f'flame_2023_{environment}/tracked_flame_params_100.npz'))\n",
        "\n",
        "  return {\n",
        "      'rgb': rgb,\n",
        "      'segmentation': segmentation,\n",
        "      'foreground_mask': foreground_mask,\n",
        "      'camera': camera,\n",
        "      'flame_mesh': flame_mesh,\n",
        "      'flame_params': flame_params,\n",
        "      }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHiBkN128hL-"
      },
      "source": [
        "## Visualizations\n",
        "We first visualize all modalities for a single example and then plot multiple cameras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mxwLs3DzllJ"
      },
      "source": [
        "### Individual Scene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvP7niU9v8OJ"
      },
      "outputs": [],
      "source": [
        "#@title Choose Example\n",
        "base_dir = './'\n",
        "# Load the following scene:\n",
        "subject = '00000'  #@param{\"type\": \"string\"}\n",
        "expression = '00001'  #@param{\"type\": \"string\"}\n",
        "environment= '002'  #@param{\"type\": \"string\"}\n",
        "camera_name = 'C00'  #@param{\"type\": \"string\"}\n",
        "\n",
        "example = load_example(base_dir, subject, expression, environment, camera_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JKmZV3Tu993"
      },
      "outputs": [],
      "source": [
        "#@title Visualize Scene\n",
        "visuals_keys = ['rgb', 'segmentation', 'foreground_mask']\n",
        "visuals = {k: example[k] for k in visuals_keys}\n",
        "visuals['segmentation'] = visuals['segmentation'] / np.max(visuals['segmentation'])  # Scale to [0, 1.0] for visualization\n",
        "\n",
        "mediapy.show_images(visuals.values(), titles=visuals_keys, height=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrJykGwZ8dqj"
      },
      "outputs": [],
      "source": [
        "#@title Visualize Semantic Regions\n",
        "semantic_regions = set(example['segmentation'].reshape(-1).tolist())\n",
        "semantic_vis = list()\n",
        "\n",
        "# We only visualize the regions in this example. Other images have more semantic regions.\n",
        "for semantic_region in semantic_regions:\n",
        "    vis = np.zeros_like(example['segmentation'])\n",
        "    vis[example['segmentation'] == semantic_region] = 255\n",
        "    semantic_vis.append(vis)\n",
        "mediapy.show_images(semantic_vis, titles=[f'Region {region}' for region in semantic_regions], height=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUiJLHESzllK"
      },
      "outputs": [],
      "source": [
        "#@title List Camera Fields\n",
        "for k in example['camera']:\n",
        "  print(k, np.array(example['camera'][k]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title List FLAME Parameters\n",
        "\n",
        "# The dict contains parameters for all 13 expression.\n",
        "for k in example['flame_params']:\n",
        "  print(k, example['flame_params'][k].shape)"
      ],
      "metadata": {
        "id": "JrWSoKrrN3fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize Cameras for Multiple View Points\n",
        "n_cameras = 30  #@param{\"type\": \"number\"}\n",
        "near, far = 0, 0.3\n",
        "\n",
        "camera_names = [f'C{i:02d}' for i in range(n_cameras)]\n",
        "examples = list(map(lambda camera_name: load_example(base_dir, subject, expression, environment, camera_name), camera_names))\n",
        "\n",
        "positions = np.array([example['camera']['position'] for example in examples])\n",
        "orientations = np.array([example['camera']['orientation'] for example in examples])\n",
        "orientations_x = orientations[:, 0]\n",
        "orientations_y = orientations[:, 1]\n",
        "orientations_z = orientations[:, 2]\n",
        "\n",
        "cam_x = (np.linspace(near, far, 10)[:, None, None] * orientations_x + positions).reshape(-1, 3)\n",
        "cam_y = (np.linspace(near, far, 10)[:, None, None] * orientations_y + positions).reshape(-1, 3)\n",
        "cam_z = (np.linspace(near, far, 10)[:, None, None] * orientations_z + positions).reshape(-1, 3)\n",
        "\n",
        "mesh = example['flame_mesh']\n",
        "\n",
        "origin = np.zeros((1, 3))\n",
        "fig = go.Figure(data=[ # original h3ds\n",
        "    scatter3d(positions, name='Position'),\n",
        "    scatter3d(origin, name='Origin'),\n",
        "    scatter3d(cam_x, name='X', marker={'size': 3, 'opacity': 0.5}),\n",
        "    scatter3d(cam_y, name='Y', marker={'size': 3, 'opacity': 0.5}),\n",
        "    scatter3d(cam_z, name='Z', marker={'size': 3, 'opacity': 0.5}),\n",
        "    scatter3d(mesh.vertices[::2], name='mesh', marker={'size': 1, 'opacity': 1}),\n",
        "    ])\n",
        "\n",
        "fig.update_layout(scene_camera=dict(\n",
        "    eye=dict(x=0, y=0, z=2),\n",
        "    center=dict(x=0, y=0, z=0),\n",
        "    up=dict(x=0, y=1, z=0)\n",
        "))\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "dwEntTP5FwFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V68P2-7fzllK"
      },
      "source": [
        "### Multiple Scenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB4Zo7Ne32X2"
      },
      "outputs": [],
      "source": [
        "#@title Visuals for a Neutral Expression in Three Environments\n",
        "subject = '00001'  #@param{\"type\": \"string\"}\n",
        "n_cameras = 5  #@param{\"type\": \"number\"}\n",
        "camera_names = [f'C{i:02d}' for i in range(n_cameras)]\n",
        "subject_dir = os.path.join(base_dir, subject)\n",
        "rgbs = list()\n",
        "\n",
        "for expression, env in [('00000', '000'), ('00000', '001'), ('00000', '002')]:\n",
        "  examples = list(map(lambda camera_name: load_example(base_dir, subject, expression, env, camera_name), camera_names[:n_cameras]))\n",
        "  rgbs += [example['rgb'] for example in examples]\n",
        "\n",
        "mediapy.show_images(rgbs, height=256, columns=n_cameras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EncHlo8e36eZ"
      },
      "outputs": [],
      "source": [
        "#@title Visuals for One Subject with an Expressive Face in the Same Environment\n",
        "subject = '00001'  #@param{\"type\": \"string\"}\n",
        "n_cameras = 5  #@param{\"type\": \"number\"}\n",
        "rgbs = list()\n",
        "\n",
        "for expression, env in [('00001', '000'), ('00002', '000'), ('00003', '000')]:\n",
        "  examples = list(map(lambda camera_name: load_example(base_dir, subject, expression, env, camera_name), camera_names[:n_cameras]))\n",
        "  rgbs += [example['rgb'] for example in examples]\n",
        "\n",
        "mediapy.show_images(rgbs, height=256, columns=n_cameras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY8hNKOIzllL"
      },
      "outputs": [],
      "source": [
        "#@title Visuals for Multiple Subjects, Expressions, and Environments\n",
        "n_subjects = 3  #@param{\"type\": \"number\"}\n",
        "n_expressions = 3  #@param{\"type\": \"number\"}\n",
        "n_environments = 3  #@param{\"type\": \"number\"}\n",
        "n_cameras = 5  #@param{\"type\": \"number\"}\n",
        "\n",
        "for subject_i in range(n_subjects):\n",
        "    rgbs = list()\n",
        "    for expression_i in range(n_expressions):\n",
        "        for env_i in range(n_environments):\n",
        "          examples = list(map(lambda camera_name: load_example(base_dir, f'{subject_i:05d}', f'{expression_i:05d}', f'{env_i:03d}', camera_name), camera_names[:n_cameras]))\n",
        "          rgbs += [example['rgb'] for example in examples]\n",
        "    print(f'Subject {subject_i:05d}')\n",
        "    mediapy.show_images(rgbs, height=256, columns=n_cameras)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmP_-UJpPorJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "u17eRANh0hnm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}