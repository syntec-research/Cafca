{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cafca Synthetic Dataset"
      ],
      "metadata": {
        "id": "0Iaq4ION0UYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Information"
      ],
      "metadata": {
        "id": "u17eRANh0hnm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh6jBVXz9Bo2"
      },
      "source": [
        "\n",
        "This notebook visualizes the main components of the Cafca synthetic dataset. The full dataset contains 1,500 subjects, each of which is rendered with 13 expressions in 3 environments from 30 views, resulting in 1.755 Mio. images.\n",
        "\n",
        "### Folder Structure\n",
        "\n",
        "The paths follow this format: `SUBJECT`/`EXP`_`ENV`/`{cameras_json, color_image, foreground_mask, segmentation}`\n",
        "\n",
        "`SUBJECT` and `EXP` are 5-digit numbers. `ENV` is a 3-digit number. Camera names go from `C00` to `C29`.\n",
        "\n",
        "Please see \"Utilities\" (`load_example(...)`) for loading a scene.\n",
        "\n",
        "\n",
        "### Environments\n",
        "The dataset contains each expression rendered in three different environments. The first environment (index `000`) is the same for all expressions (`Laval_Indoor_9C4A5690_8k.exr`). The other two environments are picked at random from the [Laval Indoor Dataset](http://indoor.hdrdb.com/). The environment name is saved in `environment.json` in the `color_image` folder.\n",
        "\n",
        "\n",
        "### Coordinate System\n",
        "The dataset uses a right-handed coordinate system. For convenience, the camera json file contains the full projection matrices (`P`), extrinsic and intrinsic matrices (`world2cam` / `cam2world` and `K`), and all of thes parameters individually."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "VqNvPc4D0w8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and Unzip Sample Dataset\n",
        "print('Downloading sample dataset with 5 identities...')\n",
        "!wget https://dataset.ait.ethz.ch/downloads/cafca/mini_sample_dataset.zip\n",
        "!unzip mini_sample_dataset.zip"
      ],
      "metadata": {
        "id": "lTAG2bvQDZZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapy"
      ],
      "metadata": {
        "id": "4KBYnM2vE0bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt_dI-oiuux-"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import plotly.io as pio\n",
        "\n",
        "import mediapy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import plotly.graph_objs as go\n",
        "from typing import List, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zqBvzk3uyy2"
      },
      "outputs": [],
      "source": [
        "#@title Utilities\n",
        "\n",
        "def read_camera_json(path: str) -> dict[str, Any]:\n",
        "  \"\"\"Returns a dict with camera parameters.\"\"\"\n",
        "  with open(path, 'rb') as f:\n",
        "    return json.load(f)\n",
        "\n",
        "\n",
        "def read_foreground_mask(path: str) -> np.ndarray:\n",
        "  \"\"\"Returns a float foreground mask file in [0, 1.0].\"\"\"\n",
        "  foreground_mask = mediapy.read_image(path).astype(float)\n",
        "  # Map from [0, 255] to [0, 1].\n",
        "  return foreground_mask / 255.0\n",
        "\n",
        "\n",
        "def scatter3d(arr: np.ndarray | List, name: str, mode: str='markers', **kwargs) -> go.Scatter3d:\n",
        "  \"\"\"Returns a plotly.graph_objs.Scatter3d object for the given array.\"\"\"\n",
        "  arr = np.array(arr)\n",
        "  return go.Scatter3d(x=arr[:, 0], y=arr[:, 1], z=arr[:, 2], mode=mode, name=name, **kwargs)\n",
        "\n",
        "\n",
        "def load_example(frame_dir: str, camera_name: str) -> dict[str, Any]:\n",
        "  \"\"\"Returns a dict with data for an individual scene.\"\"\"\n",
        "  rgb = mediapy.read_image(os.path.join(frame_dir, f'color_image/{camera_name}.png'))\n",
        "  segmentation = mediapy.read_image(os.path.join(frame_dir, f'segmentation/{camera_name}.png'))\n",
        "  foreground_mask = read_foreground_mask(os.path.join(frame_dir, f'foreground_mask/{camera_name}.png'))\n",
        "  camera = read_camera_json(os.path.join(frame_dir, f'cameras_json/{camera_name}.json'))\n",
        "  return {\n",
        "      'rgb': rgb,\n",
        "      'segmentation': segmentation,\n",
        "      'foreground_mask': foreground_mask,\n",
        "      'camera': camera,\n",
        "      }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHiBkN128hL-"
      },
      "source": [
        "## Visualizations\n",
        "We first visualize all modalities for a single example and then plot multiple cameras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mxwLs3DzllJ"
      },
      "source": [
        "### Individual Scene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvP7niU9v8OJ"
      },
      "outputs": [],
      "source": [
        "#@title Choose Example\n",
        "base_dir = 'mini_sample_dataset'\n",
        "# Load the following scene:\n",
        "subject = '00000'  #@param{\"type\": \"string\"}\n",
        "expression = '00001'  #@param{\"type\": \"string\"}\n",
        "environment= '002'  #@param{\"type\": \"string\"}\n",
        "camera_name = 'C00'  #@param{\"type\": \"string\"}\n",
        "\n",
        "frame_dir = os.path.join(base_dir, subject, f'{expression}_{environment}')\n",
        "example = load_example(frame_dir, camera_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JKmZV3Tu993"
      },
      "outputs": [],
      "source": [
        "#@title Visualize Scene\n",
        "visuals_keys = ['rgb', 'segmentation', 'foreground_mask']\n",
        "visuals = {k: example[k] for k in visuals_keys}\n",
        "visuals['segmentation'] = visuals['segmentation'] / np.max(visuals['segmentation'])  # Scale to [0, 1.0] for visualization\n",
        "\n",
        "mediapy.show_images(visuals.values(), titles=visuals_keys, height=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrJykGwZ8dqj"
      },
      "outputs": [],
      "source": [
        "#@title Visualize Semantic Regions\n",
        "semantic_regions = set(example['segmentation'].reshape(-1).tolist())\n",
        "semantic_vis = list()\n",
        "for semantic_region in semantic_regions:\n",
        "    vis = np.zeros_like(example['segmentation'])\n",
        "    vis[example['segmentation'] == semantic_region] = 255\n",
        "    semantic_vis.append(vis)\n",
        "mediapy.show_images(semantic_vis, titles=[f'Region {region}' for region in semantic_regions], height=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUiJLHESzllK"
      },
      "outputs": [],
      "source": [
        "#@title List Camera Fields\n",
        "example['camera'].keys()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize Cameras for Multiple View Points\n",
        "n_cameras = 30  #@param{\"type\": \"number\"}\n",
        "near, far = 0, 0.3\n",
        "\n",
        "camera_names = [f'C{i:02d}' for i in range(n_cameras)]\n",
        "examples = list(map(lambda camera_name: load_example(frame_dir, camera_name), camera_names))\n",
        "\n",
        "positions = np.array([example['camera']['position'] for example in examples])\n",
        "orientations = np.array([example['camera']['orientation'] for example in examples])\n",
        "orientations_x = orientations[:, 0]\n",
        "orientations_y = orientations[:, 1]\n",
        "orientations_z = orientations[:, 2]\n",
        "\n",
        "cam_x = (np.linspace(near, far, 10)[:, None, None] * orientations_x + positions).reshape(-1, 3)\n",
        "cam_y = (np.linspace(near, far, 10)[:, None, None] * orientations_y + positions).reshape(-1, 3)\n",
        "cam_z = (np.linspace(near, far, 10)[:, None, None] * orientations_z + positions).reshape(-1, 3)\n",
        "\n",
        "origin = np.zeros((1, 3))\n",
        "fig = go.Figure(data=[ # original h3ds\n",
        "    scatter3d(positions, name='Position'),\n",
        "    scatter3d(origin, name='Origin'),\n",
        "    scatter3d(cam_x, name='X', marker={'size': 3, 'opacity': 0.5}),\n",
        "    scatter3d(cam_y, name='Y', marker={'size': 3, 'opacity': 0.5}),\n",
        "    scatter3d(cam_z, name='Z', marker={'size': 3, 'opacity': 0.5}),\n",
        "    ])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "dwEntTP5FwFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V68P2-7fzllK"
      },
      "source": [
        "### Multiple Scenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB4Zo7Ne32X2"
      },
      "outputs": [],
      "source": [
        "#@title Visuals for a Neutral Expression in Three Environments\n",
        "subject = '00001'  #@param{\"type\": \"string\"}\n",
        "n_cameras = 5  #@param{\"type\": \"number\"}\n",
        "camera_names = [f'C{i:02d}' for i in range(n_cameras)]\n",
        "subject_dir = os.path.join(base_dir, subject)\n",
        "rgbs = list()\n",
        "\n",
        "for expression, env in [('00000', '000'), ('00000', '001'), ('00000', '002')]:\n",
        "  frame_dir = os.path.join(subject_dir, f'{expression}_{env}/')\n",
        "  examples = list(map(lambda camera_name: load_example(frame_dir, camera_name), camera_names[:n_cameras]))\n",
        "  rgbs += [example['rgb'] for example in examples]\n",
        "\n",
        "mediapy.show_images(rgbs, height=256, columns=n_cameras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EncHlo8e36eZ"
      },
      "outputs": [],
      "source": [
        "#@title Visuals for One Subject with an Expressive Face in the Same Environment\n",
        "subject = '00001'  #@param{\"type\": \"string\"}\n",
        "subject_dir = os.path.join(base_dir, subject)\n",
        "n_cameras = 5  #@param{\"type\": \"number\"}\n",
        "subject_dir = os.path.join(base_dir, subject)\n",
        "rgbs = list()\n",
        "\n",
        "for expression, env in [('00001', '000'), ('00002', '000'), ('00003', '000')]:\n",
        "  frame_dir = os.path.join(subject_dir, f'{expression}_{env}/')\n",
        "  examples = list(map(lambda camera_name: load_example(frame_dir, camera_name), camera_names[:n_cameras]))\n",
        "  rgbs += [example['rgb'] for example in examples]\n",
        "\n",
        "mediapy.show_images(rgbs, height=256, columns=n_cameras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY8hNKOIzllL"
      },
      "outputs": [],
      "source": [
        "#@title Visuals for Multiple Subjects, Expressions, and Environments\n",
        "n_subjects = 3  #@param{\"type\": \"number\"}\n",
        "n_expressions = 3  #@param{\"type\": \"number\"}\n",
        "n_environments = 3  #@param{\"type\": \"number\"}\n",
        "n_cameras = 5  #@param{\"type\": \"number\"}\n",
        "\n",
        "for subject_i in range(n_subjects):\n",
        "    rgbs = list()\n",
        "    for expression_i in range(n_expressions):\n",
        "        for env_i in range(n_environments):\n",
        "          frame_dir = os.path.join(base_dir, f'{subject_i:05d}', f'{expression_i:05d}_{env_i:03d}/')\n",
        "          examples = list(map(lambda camera_name: load_example(frame_dir, camera_name), camera_names[:n_cameras]))\n",
        "          rgbs += [example['rgb'] for example in examples]\n",
        "    print(f'Subject {subject_i:05d}')\n",
        "    mediapy.show_images(rgbs, height=256, columns=n_cameras)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "u17eRANh0hnm"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}